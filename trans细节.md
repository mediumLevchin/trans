#### token 
[输入Padding](https://deepinout.com/pytorch/pytorch-questions/121_pytorch_how_does_max_length_padding_and_truncation_arguments_work_in_huggingface_berttokenizerfastfrom_pretrainedbertbaseuncased.html)
[LLM 如何token](https://www.zhihu.com/question/64984731)
[BPE主要用在拉丁语系，前后缀](https://blog.csdn.net/weixin_36378508/article/details/134883614)  
[BPE主要是GPT WordPiece主要是Bert](https://cloud.tencent.com/developer/article/1865689)

#### Trans细节
[用qkv,不直接使用X，增强拟合能力](https://lulaoshi.info/deep-learning/attention/transformer-attention.html#self-attention%E4%B8%AD%E7%9A%84q%E3%80%81k%E3%80%81v)  
[可以不用Musk，直接表示成n+1个样本](https://www.bilibili.com/read/cv20387281/)  
trans 不同block的q参数不一样
[将Q、K内积表示成Xm,Xn,m-n的函数，旋转位置编码使用旋转矩阵构造。实验中表现出很好的外推性](https://cloud.tencent.com/developer/article/2327751)  
[d是行数还是列数，NLP&DL的糟粕之一是数学符号滥用](https://blog.csdn.net/weixin_45610907/article/details/128175273)  
[训练和推理区别](https://zhuanlan.zhihu.com/p/622714425)  
[decoder推理时的输入](https://www.zhihu.com/question/337886108)  
[训练和推理区别2](https://zhuanlan.zhihu.com/p/707926896)  

#### bert和gpt
[bert预训练细节]
